{"name":"Swift-hadoop","tagline":"Swift workflows on Hadoop","body":"# Swift on Hadoop\r\n\r\n### Get swift installed\r\n\r\n* Download the swift package from http://swift-lang.org/downloads/index.php\r\n* Untar swift package : tar -xzf swift-0.94.1.tar.gz\r\n* Add the following to .bashrc to get swift in path\r\n\r\n```bash\r\nexport PATH=$PWD/swift-0.94.1/bin:$PATH\r\n# Check by running:\r\nswift -version\r\n```\r\n\r\n### Start workers on hadoop\r\n\r\nBefore swift scripts can be run across the hadoop cluster, swift workers must\r\nbe started on the hadoop nodes. These workers allow swift to execute its jobs\r\non the cluster. First the start_hadoop_workers.sh script must be configured\r\nfor that jobs that you would run on the cluster.\r\n\r\nHere's the configuration section of the hadoop_coasters/start_hadoop_workers.sh\r\nscript:\r\n\r\n```bash\r\n#Number of workers started by hadoop for swift\r\nWORKER_COUNT=10\r\n#Worker walltime in minutes\r\nWALLTIME=240\r\n#Remote setup script\r\nREMOTE_SCRIPT=./install_nltk.sh\r\n#Input data dir on hdfs to start workers\r\nINPUT_DIR=\"/user/$USER/tmp\"\r\n#IP of headnode\r\nHEADNODE=\"http://128.135.159.52\"\r\n# Hadoop streaming jar\r\nHADOOP_STREAMING=\"/opt/hadoop-0.20.203.0/contrib/streaming/hadoop-streaming-0.20.203.0.jar\"\r\n#Port for python webserver\r\nPUBLISH_PORT=$(( 55000 + $(($RANDOM % 1000))  ))\r\n#Port used by workers to connect back to coaster service\r\nWORKER_PORT=$((  50000 + $(($RANDOM % 1000 )) ))\r\n#Port used by swift to talk to the coaster service\r\nSERVICE_PORT=$(( 51000 + $(($RANDOM % 1000 )) ))\r\n#Output from scripts on HDFS\r\nOUTPUT=/user/$USER/results\r\n\r\n```\r\n\r\nEnsure that the WORKER_COUNT is set to a number appropriate for the size of\r\nthe jobs that you wish to run. Since swift workers fork off tasks submitted to it,\r\nand the node is generally a commodity system, the nature of the app would generally\r\ndetermine how many cores it uses.\r\n\r\nThe WALLTIME is a hard limit applied to both swift workers as well as the hadoop\r\njob which starts up the workers. As a result, ensure that the WALLTIME exceeds the\r\nlongest duration your jobs may run with the resources requested.\r\n\r\nREMOTE_SCRIPT points to bash script, which is shipped to the hadoop nodes, and executed\r\nbefore the workers initialise. Node specific installations and data setups can be done\r\nusing this script. These setups generally persist unless you have added a mechanism to\r\ndo cleanups.\r\n\r\nHEADNODE should be set to either a network accessible URL of the headnode or the IP of\r\nthe headnode, where you'd be running Swift from. Prefix the URL/IP with \"http://\".\r\nThe default is set to the address of the OCCY-hadoop cluster\r\n\r\nHADOOP_STREAMING should be set to the full path to the hadoop streaming jar file on the\r\nhadoop headnode. By default this is set to point to the jar file on the OCCY-hadoop\r\ncluster.\r\n\r\nINPUT_DIR, should be set to a temporary directory on the HDFS.\r\n\r\nEnsure that the PUBLISH_PORT, WORKER_PORT and SERVICE_PORT are all withing ranges that\r\nare not blocked by firewalls.\r\n\r\nOnce you've setup the configurations,\r\n\r\n```bash\r\n# start workers :\r\n./hadoop_coasters/start_hadoop_workers.sh\r\n```\r\n\r\n### Run swift workflows.\r\n\r\n* Update run-swift.sh for your configs\r\n* Edit stable at your installation of swift-0.94.1\r\n* Point DIR at the default dataset you want to use\r\n* CONFIG variable should by default be set to \"hadoop\" on OCCY.\r\n\r\n```bash\r\n#To run:\r\n./run-swift.sh /relative/path/to/your-dataset\r\n```\r\n\r\n### Advanced modifications\r\n\r\nThe hadoop worker submission commandline can be modified with to do several\r\ninteresting and useful things. Here are some examples:\r\n\r\nEnvironment variables can be set on the swift workers by passing them through\r\nthe hadoop job which starts the workers. The environment is inherited by the\r\ntasks run on the nodes by swift.\r\n\r\n```bash\r\n# Add argument to the hadoop job call\r\n-cdmenv <VAR>=<values>\r\n```\r\n\r\nFiles, can be moved the workers sandbox directory using the -file option. This can be\r\nused to stage installation packages easily.\r\n```bash\r\n# Add argument to the hadoop job call\r\n-file <file>\r\n```\r\nThere is a python webserver that is started by default and the address to that\r\nwebserver is set to an environment variable PYWEBSERVER on all workers. This\r\ncan be used to download files from the current directory on the headnode.\r\n\r\n### Cleanup.\r\n\r\nYou might see swift errors when running from a directory with previous results.\r\nYou could clean the whole directory by running :\r\n\r\n```bash\r\n# Cleanup script\r\n./clean.sh\r\n```\r\n\r\n### Need help ?\r\n\r\nIf you run into anything unexpected, please email <swift-user@ci.uchicago.edu>\r\n\r\n### TODO LIST\r\n\r\n* Re-order the setup to be app independent and maintain separate branch for KLAB\r\n* Documentation cleanup\r\n  ** Document worker starting mechanim\r\n  ** Coasters troubleshooting\r\n  ** Links to swift install and tutorial","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}